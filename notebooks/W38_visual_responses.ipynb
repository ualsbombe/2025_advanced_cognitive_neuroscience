{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15f9bff",
   "metadata": {},
   "source": [
    "## TUTORIAL W38\n",
    "\n",
    "In this tutorial, we will go through how to analyse visual responses.\n",
    "Steps will include:  \n",
    "- preprocessing\n",
    "     - artefact rejection\n",
    "     - filtering\n",
    "- epoching\n",
    "     - rejecting based on peak-to-peak amplitude\n",
    "- evoked responses\n",
    "      - difference waves\n",
    "- estimating noise covariance\n",
    "      - whitening the data\n",
    "- do an inverse model (MNE)\n",
    "- extract labels from the cerebral cortex\n",
    "    - plot source times courses from different labels\n",
    "    - do difference plots\n",
    "- do a multivariate analysis in source space\n",
    "    - (doing it in sensor space is left as an exercise to the reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d81679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS AND DEFAULT PLOTTING PARAMETERS\n",
    "\n",
    "import mne ## MNE-Python for analysing data\n",
    "## below magic provides interactive plots in notebook\n",
    "%matplotlib widget\n",
    "from os import chdir\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt ## for basic plotting\n",
    "import matplotlib as mpl ## for setting default parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e3c3c",
   "metadata": {},
   "source": [
    "## SAMPLE DATA SET (https://mne.tools/stable/documentation/datasets.html#sample)\n",
    "*These data were acquired with the Neuromag Vectorview system at MGH/HMS/MIT Athinoula A. Martinos Center Biomedical Imaging. EEG data from a 60-channel electrode cap was acquired simultaneously with the MEG. The original MRI data set was acquired with a Siemens 1.5 T Sonata scanner using an MPRAGE sequence.*\n",
    "\n",
    "*In this experiment, checkerboard patterns were presented to the subject into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.*\n",
    "\n",
    "Change the path to your relevant path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LOAD SAMPLE DATA SET\n",
    "\n",
    "sample_path = '/work/MEG_data/MNE-sample-data' ## UCloud\n",
    "sample_meg_path = join(sample_path, 'MEG', 'sample')\n",
    "chdir(sample_meg_path)\n",
    "subjects_dir = '../../subjects/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d36da3",
   "metadata": {},
   "source": [
    "## MARKING BAD CHANNELS\n",
    "\n",
    "First try to identify the two bad channels, one electrode and one planar gradiometer.  \n",
    "(They have been greyed out; also notice that you can mark channels as bad by clicking them and \"unbadding\" them by clicking it again)\n",
    "Do notice that the two marked channels look considerably different than the others  \n",
    "The bad channels can also be found by `raw.info['bads']`\n",
    "The x-axis has time and the y-axes, magnetic field (T), magnetic gradient (T/m) or voltage (V)\n",
    "This is your $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% READ RAW\n",
    "\n",
    "raw = mne.io.read_raw_fif('sample_audvis_raw.fif', preload=True)\n",
    "fig = raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9e737",
   "metadata": {},
   "source": [
    "## IDENTIFYING BAD CHANNELS FROM THE POWER SPECTRAL DENSITY (PSD)\n",
    "\n",
    "Now we will see the two bad channels in the power spectral density.\n",
    "Describe to yourself how they differ from the rest. Often bad channels are easier to identify here than in the raw traces\n",
    "The X-axis contains the frequency (Hz) and the y-axis the power of each for each frequency in dB. Here you can also click and toggle bad channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.compute_psd().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a34e7",
   "metadata": {},
   "source": [
    "## FILTERING\n",
    "\n",
    "With filtering, we can reduce the contributions of frequencies that contain signal that is not of interest to our analysis.\n",
    "For each of the copies, compute a psd and ascertain for yourself what they do and not do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_lowpass = raw.copy() ## create a copy so we do not overwrite the original\n",
    "copy_lowpass.filter(h_freq=40,   l_freq=None) ## lowpass filter of 40 Hz\n",
    "\n",
    "copy_highpass = raw.copy()\n",
    "copy_highpass.filter(h_freq=None, l_freq=1) ## highpass filter of 1 Hz\n",
    "\n",
    "copy_bandstop = raw.copy()\n",
    "copy_bandstop.filter(h_freq=1,    l_freq=40) ## bandstop filler of 1-40 Hz\n",
    "\n",
    "copy_bandpass = raw.copy()\n",
    "copy_bandpass.filter(h_freq=40,   l_freq=1); ## bandpass fillter of 1-40 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2049b9e",
   "metadata": {},
   "source": [
    "## CHOOSE ONE OF THE FILTERS - AND THEN FIND THE EVENTS\n",
    "\n",
    "Choose a filter and apply it to your `raw` before you go further\n",
    "For the events, we will use the two checkerboard stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b25f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% FIND EVENTS\n",
    "\n",
    "events = mne.find_events(raw)\n",
    "\n",
    "## LA: 1: Response to left-ear auditory stimulus (a tone)\n",
    "## RA: 2: Response to right-ear auditory stimulus\n",
    "## LV: 3: Response to left visual field stimulus (checkerboard)\n",
    "## RV: 4: Response to right visual field stimulus\n",
    "## smiley: 5: Response to the smiley face\n",
    "## button: 32: Response triggered by the button press\n",
    "# https://mne.tools/stable/overview/datasets_index.html#sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6393",
   "metadata": {},
   "source": [
    "## CREATING EPOCHS\n",
    "\n",
    "Create epochs using `mne.Epochs`. Specifically, create two epochs_objects: `epochs` and `epochs_eog_cleaned`.\n",
    "- In both:\n",
    "    - include 200 ms before each visual event and 550 ms after each event.\n",
    "    - make sure to include both checkerboard stimuli epochs\n",
    "    - do the demeaning by creating a baseline interval from 200 ms to 0 ms.\n",
    "    - apply the `set_eeg_reference(projection=True)` to both\n",
    "- In `epochs_eog_cleaned`:\n",
    "    - include a `reject` dict that removes epochs where the peak-to-peak amplitude of eye-related activity (EOG) exceeds 250 ÂµV\n",
    "Check out https://mne.tools/stable/generated/mne.Epochs.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.Epochs()\n",
    "epochs_eog_cleaned = mne.Epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88878d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING AVERAGES AND DIFFERENCE WAVES\n",
    "\n",
    "evokeds = list()\n",
    "evokeds_eog_cleaned = list()\n",
    "for event in epochs.event_id:\n",
    "    evokeds.append(epochs[event].average())\n",
    "    evokeds_eog_cleaned.append(epochs_eog_cleaned[event].average())\n",
    "\n",
    "evoked_diff = evokeds[0].copy() # create a copy\n",
    "evoked_diff.data -= evokeds[1].data # modify the data in place\n",
    "evoked_diff.comment = 'LV - RV'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd4d33",
   "metadata": {},
   "source": [
    "## PLOT THE EVOKEDS AND THEIR DIFFERENCE WAVES\n",
    "Find out where the differences are the greatest - what does this indicate\n",
    "Use `mne.viz.plot_evoked`, `mne.viz.plot_evoked_topo`, `mne.viz.plot_evoked_topomap`, `mne.viz.plot_evoked_joint`, `mne.viz.plot_evoked_image` to get a feel for the data.\n",
    "\n",
    "Also create a difference wave `evokeds_eog_cleaned_diff`\n",
    "And make a difference of the differences between `evokeds_diff` and `evokeds_ego_cleaned_diff` - on which sensors are the differences most pronounced?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4271410",
   "metadata": {},
   "source": [
    "## FORWARD MODEL\n",
    "\n",
    "$\\boldsymbol L (\\boldsymbol r)$ is our forward model that for each source location $\\boldsymbol r$, expresses how that source is linked to the sensors. The SI-unit is $\\frac {T} {Am}$.  \n",
    "The SI-unit for the magnetic field at each time point, $t$, $\\boldsymbol b (t)$ is $T$.  \n",
    "The SI-unit for the current density $\\boldsymbol s(\\boldsymbol r, t)$  at each location $\\boldsymbol r$ and time point $t$ is $Am$.  \n",
    "The forward model states for each source at whatever $\\boldsymbol r$ how its activation in $Am$ is linked to the magnetic field at each sensor, e.g. $b_1(t)$.  \n",
    "$\\boldsymbol n(t)$ is the Gaussian noise at each time point\n",
    "\n",
    "$\\boldsymbol b(t) = \\left[\n",
    "\\begin{array}{c} \n",
    "b_1(t) \\\\\n",
    "b_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "b_M(t)\n",
    "\\end{array}\n",
    "\\right]$  \n",
    "$\\boldsymbol{b}(t) = \\boldsymbol{L}(\\boldsymbol{r}) \\boldsymbol s(\\boldsymbol r, t) + \\boldsymbol n(t)$\n",
    "\n",
    "The nice people from MNE-Python have already made a forward model for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ad558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ FORWARD MODEL\n",
    "fwd = mne.read_forward_solution('sample_audvis-meg-eeg-oct-6-fwd.fif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08259d",
   "metadata": {},
   "source": [
    "##  THE MINIMUM NORM ESTIMATE\n",
    "\n",
    "$$\n",
    "\\huge \\hat{\\boldsymbol \\nu}_{vox}(t) = \\boldsymbol L_V^T(\\boldsymbol G + \\epsilon \\boldsymbol I)^{-1} \\boldsymbol b(t)    \n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\huge \\boldsymbol G = \\int_\\Omega \\boldsymbol L (\\boldsymbol r) \\boldsymbol L^T (\\boldsymbol r) d^3r\n",
    "$$\n",
    "and with\n",
    "$$ \n",
    "\\huge\n",
    "\\boldsymbol{\\hat{\\nu}}_{vox}(t) = \\left[\n",
    "\\begin{array}{c} \n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_1, t) \\\\\n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_2, t) \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_N, t)\n",
    "\\end{array}\n",
    "\\right]  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668ec0e",
   "metadata": {},
   "source": [
    "## CREATE SOURCE TIME COURSES FOR EACH CONDITION\n",
    "\n",
    "Create `stc` (`stc = mne.minimum_norm.apply_inverse`) from `evokeds` , separate ones for each event ((LV and RV))\n",
    "- Do you think you would see more frontal activity in `stc` compared to `stc_eog_cleaned`? And why is that?\n",
    "- For the noise covariance plots, which of three kinds of sensors show the highest correlations between sensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  WHITEN the data, i.e. normalizing magnetometers, gradiometers\n",
    "## and electrode readings to make them comparable\n",
    "noise_cov = mne.compute_covariance(epochs, tmin=None, tmax=0)\n",
    "noise_cov.plot(raw.info)\n",
    "\n",
    "inverse_operator = mne.minimum_norm.make_inverse_operator(epochs.info, fwd,\n",
    "                                                          noise_cov)\n",
    "\n",
    "# estimating the source pattern for each time point Vvox(t)\n",
    "# use the MNE method and not the default dSPM\n",
    "stc = mne.minimum_norm.apply_inverse(method='MNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffec4b",
   "metadata": {},
   "source": [
    "## EXTRACT RELEVANT LABELS FROM THE CEREBRAL CORTEX\n",
    "\n",
    "go to your path `MNE-sample-data/subjects/sample/label`, this is segmented brain data, and read some relevant labels using `mne.read_label` from there:\n",
    "That could be:\n",
    " - `lh.V1.label`\n",
    " - `lh.V2.label`\n",
    " - `rh.V1.label`\n",
    " - `rh.V2.label`\n",
    "\n",
    "You can read in even more labels by running `labels = mne.read_labels_from_annot('sample', subjects_dir=subjects_dir)`\n",
    "\n",
    "Plot some (visual) labels that should show a response, using `plt.plot` from `matplotlib`. Here's some code that could get you started:\n",
    "\n",
    "```\n",
    "lh_V1_label = mne.read_label(<path_to_lh_V1_label>)\n",
    "stc_lh_V1_label = stc.in_label(lh_V1_label)\n",
    "plt.figure()\n",
    "plt.plot(stc_lh_V1_label.times, stc_lh_V1_label.data.T)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Current density (Am)')\n",
    "plt.title(lh_V1_label.name)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "Also plot some labels that should not show a response (maybe some language related labels)\n",
    "Discuss with each other what the lines each represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8642e",
   "metadata": {},
   "source": [
    "##  MACHINE LEARNING LOGISTIC REGRESSION \n",
    "\n",
    "Finally, we are going to do some machine learning on this data, trying to predict whether the checkerboard was shown in the left visual field or the right visual field. For this we need to do source reconstruction of the epochs instead.\n",
    "\n",
    "\n",
    "Below is some code to get you started\n",
    "\n",
    "```\n",
    "\n",
    "## GETTING SOURCE TIME COURSES FROM EPOCHS\n",
    "\n",
    "stcs = mne.minimum_norm.apply_inverse_epochs(epochs, lambda=1, method='MNE')\n",
    "\n",
    "## READING LABELS FROM ANNOTATION\n",
    "labels = mne.read_labels_from_annot('sample', subjects_dir=subjects_dir)\n",
    "\n",
    "## EXTRACT LABELS FROM MULTIPLE STCS\n",
    "def extract_label(label, stcs):\n",
    "    label_stcs = list()\n",
    "    for stc in stcs:\n",
    "        label_stcs.append(stc.in_label(label))\n",
    "        \n",
    "    return label_stcs\n",
    "\n",
    "\n",
    "#%% CREATE X AND Y FOR LOGISTIC REGRESSIOM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = epochs.events[:, 2] # the codes for the visual events\n",
    "\n",
    "n_events = len(y) ## how many repetitions\n",
    "n_samples = label_stcs[0].data.shape[1] # how many time points\n",
    "n_label_vertices = label_stcs[0].data.shape[0]  # how many source positions\n",
    "\n",
    "X = np.zeros(shape=(n_events, n_label_vertices, n_samples))\n",
    "\n",
    "## assign data to X\n",
    "for event_index in range(n_events):\n",
    "    X[event_index, :, :] = label_stcs[event_index].data\n",
    "    \n",
    "    \n",
    "#%% DO LOGISTIC REGRESSION PER TIME SAMPLE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scores_list_samples = [None] * n_samples\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "for sample_index in range(n_samples):\n",
    "    print(sample_index)\n",
    "    this_X = X[:, :, sample_index]\n",
    "    this_X_std = sc.fit_transform(this_X) ## standardise the data\n",
    "    logr = LogisticRegression(C=1e-3)\n",
    "    scores_list_samples[sample_index] = np.mean(cross_val_score(logr,\n",
    "                                                                this_X_std,\n",
    "                                                                y,\n",
    "                                                        cv=StratifiedKFold()))\n",
    "\n",
    "\n",
    "## PLOTTING THE CLASSIFICATION ACCURACY\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stc.times, scores_list_samples)\n",
    "plt.show()\n",
    "plt.title('Classification of LV vs RV')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Proportion correctly classif')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne_acn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
