{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff05b627-2042-4f1a-8c60-2b9c37ff3600",
   "metadata": {},
   "source": [
    "# TUTORIAL W36\n",
    "\n",
    "In this tutorial, we will get acquainted with working with MEG data on UCloud  \n",
    "We will be using the MNE-sample-data that contains visual and auditory stimulations  \n",
    "The dataset also contains anatomical MR-data processed with FreeSurfer, meaning that we have access to a forward model linking the source space to sensor space.  \n",
    "With that we can create an inverse model for reconstructing the sensor space visual and auditory responses in the source space.  \n",
    "In this tutorial we will zoom through a lot of the steps, which throughout the course will be covered explicitly\n",
    "\n",
    "## SETUP YOUR ENVIRONMENT\n",
    "\n",
    "**Make sure that you have followed the instructions for setting up your environment, local and UCloud before beginning on this.** The instructions can be found here: https://github.com/ualsbombe/2025_advanced_cognitive_neuroscience/blob/main/README.md\n",
    "\n",
    "## RESPECT EACH OTHER'S PERSONAL DATA\n",
    "\n",
    "**Note** that 3D plots cannot be opened on UCloud (these are mostly (fancy) source space plots). That is why we also set up a local environment for you to create these plots. It is very important that you do **not** store any subject MR-data locally. Therefore, we will only plot on the *fsaverage* template brain (distributed with FreeSurfer). Single subjects can be morphed onto this template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af465a9-8e7a-476a-8fd5-b5b9b7ac5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import mne ## MNE-Python for analysing data\n",
    "## below magic provides interactive plots in notebook\n",
    "%matplotlib widget\n",
    "from os import chdir\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt ## for basic plotting\n",
    "import matplotlib as mpl ## for setting default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c2855-0808-44c4-97ee-bf424131712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SET DEFAULT PLOTTING PARAMETERS\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams['font.size'] = 24\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "mpl.rcParams['lines.linewidth'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dcdea-7107-4e6c-9463-3ae58bf5f055",
   "metadata": {},
   "source": [
    "## SAMPLE DATA SET (https://mne.tools/stable/documentation/datasets.html#sample)\n",
    "*These data were acquired with the Neuromag Vectorview system at MGH/HMS/MIT Athinoula A. Martinos Center Biomedical Imaging. EEG data from a 60-channel electrode cap was acquired simultaneously with the MEG. The original MRI data set was acquired with a Siemens 1.5 T Sonata scanner using an MPRAGE sequence.*\n",
    "\n",
    "*In this experiment, checkerboard patterns were presented to the subject into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.*\n",
    "\n",
    "Change the path to your relevant path below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc3952-af5b-464c-9c98-7b63470b3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LOAD SAMPLE DATA SET\n",
    "\n",
    "sample_path = '/work/MEG_data/MNE-sample-data' ## UCloud\n",
    "sample_path = '/home/lau/mne_data/MNE-sample-data/' ## local\n",
    "sample_meg_path = join(sample_path, 'MEG', 'sample')\n",
    "chdir(sample_meg_path)\n",
    "subjects_dir = '../../subjects/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94eb06d-2c27-4ae3-bb48-c4afa208f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% READ RAW\n",
    "\n",
    "raw_sample = mne.io.read_raw_fif('sample_audvis_raw.fif', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b9cc4-92cd-4a88-8d19-155ea561081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT RAW DATA\n",
    "fig = raw_sample.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab1575-71e3-41ab-84b5-939726dbc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE POWER SPECTRAL DENSITY\n",
    "psd = raw_sample.compute_psd() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052b383-03bf-4691-b64e-e2c23fb6021e",
   "metadata": {},
   "source": [
    "## INSPECT THE DATA\n",
    "\n",
    "How do you tell that an online low-pass filter of 170 Hz has been applied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913692f-8200-439f-8bc9-2e3623b7c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT POWER SPECTRAL DENSITY\n",
    "fig = psd.plot() # online low-pass filter of 170 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410851f-1389-4c67-a258-5472687948f4",
   "metadata": {},
   "source": [
    "## FILTERING THE DATA\n",
    "\n",
    "We low-pass filter the data, as we are only looking for evoked responses, which are rarely above 40 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb74ab-809a-4f77-acb5-74a8595ee8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTERING DATA WITH LOW-PASS FILTER\n",
    "raw_sample.filter(l_freq=None, h_freq=40); # low-pass filter of 40 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787db9c-f64e-4ef8-8873-309fc4fddf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTTING POWER SPECTRAL DENSITY AFTER LOW-PASS FILTER\n",
    "fig = raw_sample.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa087f3-090c-40c9-afc5-5abd763120ee",
   "metadata": {},
   "source": [
    "## FINDING THE RELEVANT EVENTS\n",
    "\n",
    "On the trigger channel `'STI014'`, square pulses were sent to indicate when stimuli were presented and responses were registered.\n",
    "Check the `raw_sample.plot()` and see if you can find it and the square pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81e98d-61d6-459b-8c3b-72e4981b6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% FIND EVENTS\n",
    "\n",
    "events_sample = mne.find_events(raw_sample)\n",
    "mne.viz.plot_events(events_sample, sfreq=raw_sample.info['sfreq']);\n",
    "\n",
    "## LA: 1: Response to left-ear auditory stimulus (a tone)\n",
    "## RA: 2: Response to right-ear auditory stimulus\n",
    "## LV: 3: Response to left visual field stimulus (checkerboard)\n",
    "## RV: 4: Response to right visual field stimulus\n",
    "## smiley: 5: Response to the smiley face\n",
    "## button: 32: Response triggered by the button press\n",
    "# https://mne.tools/stable/overview/datasets_index.html#sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2f7f3-16d1-4633-b9f4-f08b7198a6f0",
   "metadata": {},
   "source": [
    "## SEGMENTING/EPOCHING THE DATA\n",
    "\n",
    "Instead of handling all the data at once, we just cut out segments around the events of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8197e-05ff-4dd7-9708-d0b1d0fdc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% EPOCH THE DATA\n",
    "event_id = dict(LA=1) ## we'll just look at this one event\n",
    "tmin = -0.200 # s\n",
    "tmax =  0.600 # s\n",
    "baseline = (None, 0) # s (from beginning to 0); for demeaning\n",
    "\n",
    "## segment the data\n",
    "epochs_sample = mne.Epochs(raw_sample, events_sample,\n",
    "                           event_id, tmin, tmax, baseline, preload=True)\n",
    "epochs_sample.set_eeg_reference(projection=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b5f8c-3dd8-466b-a154-36b167b7aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT EPOCHS\n",
    "fig = epochs_sample.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4ab60-9ac4-425a-8cd7-98c9d77d2c77",
   "metadata": {},
   "source": [
    "## AVERAGING TO OBTAIN AN EVOKED RESPONSE\n",
    "\n",
    "To increase the signal-to-noise ratio, we average the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29ff86-786d-4c35-8cef-51f5ddea0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% AVERAGE (EVOKED)\n",
    "evoked_sample = epochs_sample.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff464c7-1054-4f55-8240-9ecf25f96962",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTTING AVERAGES\n",
    "evoked_sample.plot() \n",
    "mne.viz.plot_evoked_topo(evoked_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f2ee0-1a8b-4931-9c4f-9480540d50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HAVING MORE THAN ONE EVENT\n",
    "\n",
    "event_id = dict(LA=1, RA=2, LV=3, RV=4)\n",
    "more_epochs = mne.Epochs(raw_sample, events_sample,\n",
    "                           event_id, tmin=-0.200, tmax=0.600,\n",
    "                           baseline=(None, 0),\n",
    "                           preload=True)\n",
    "\n",
    "more_evokeds = list()\n",
    "for event in more_epochs.event_id:\n",
    "    more_evokeds.append(more_epochs[event].average());\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac67266-bae9-4342-bfac-5e330cd22cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT SEVERAL EVENTS\n",
    "mne.viz.plot_evoked_topo(more_evokeds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d47ab7-c747-447f-8a6d-f739ec24a722",
   "metadata": {},
   "source": [
    "## FORWARD MODEL\n",
    "\n",
    "$\\boldsymbol L (\\boldsymbol r)$ is our forward model that for each source location $\\boldsymbol r$, expresses how that source is linked to the sensors. The SI-unit is $\\frac {T} {Am}$.  \n",
    "The SI-unit for the magnetic field at each time point, $t$, $\\boldsymbol b (t)$ is $T$.  \n",
    "The SI-unit for the current density $\\boldsymbol s(\\boldsymbol r, t)$  at each location $\\boldsymbol r$ and time point $t$ is $Am$.  \n",
    "The forward model states for each source at whatever $\\boldsymbol r$ how its activation in $Am$ is linked to the magnetic field at each sensor, e.g. $b_1(t)$.  \n",
    "$\\boldsymbol n(t)$ is the Gaussian noise at each time point\n",
    "\n",
    "$\\boldsymbol b(t) = \\left[\n",
    "\\begin{array}{c} \n",
    "b_1(t) \\\\\n",
    "b_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "b_M(t)\n",
    "\\end{array}\n",
    "\\right]$  \n",
    "$\\boldsymbol{b}(t) = \\boldsymbol{L}(\\boldsymbol{r}) \\boldsymbol s(\\boldsymbol r, t) + \\boldsymbol n(t)$\n",
    "\n",
    "The nice people from MNE-Python have already made a forward model for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7c278-5754-49f8-8dff-40874e0251f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ FORWARD MODEL\n",
    "fwd = mne.read_forward_solution('sample_audvis-meg-eeg-oct-6-fwd.fif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c412875-6591-4a4a-b667-50eb88303a70",
   "metadata": {},
   "source": [
    "## PLOTTING THE FORWARD MODEL\n",
    "\n",
    "`mne.viz.plot_alignment` does not work on UCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc6928-33f0-47e2-ad1d-569c2d95ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT SOURCE SPACE\n",
    "\n",
    "# extract source space\n",
    "src = fwd['src']\n",
    "##plot alignment does not work on UCloud\n",
    "mne.viz.plot_alignment(info=epochs_sample.info, src=src, subjects_dir='../../subjects/',\n",
    "                       subject=\"sample\", surfaces=\"white\", trans='sample_audvis_raw-trans.fif')\n",
    "\n",
    "## 2D alternative\n",
    "\n",
    "#mne.viz.plot_sensors(raw_sample.info);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c804c-7c48-48d2-b8e0-f752e3f831ff",
   "metadata": {},
   "source": [
    "## CODE FOR GETTING INFO ABOUT THE ELEMENTS OF THE FORWARD MODEL\n",
    "\n",
    "- `info` has information about the positions of the channels in the subject's head coordinate system; the origo is at the intersection of the pre-auricular points and the nasion. The first three elements in `'loc'` are the *xyz* coordinates\n",
    "- `src` has info about where sources are placed, with the current model: on the surface of the cerebral cortex. There is also more information about the orientation of the sources.\n",
    "- `trans` contains the transformation matrix that takes the subject's head space (defined in terms of a so-called Polhemus digitisation system) into the subject's MR space, effecitively bringing the sensor positions from `info` into the same coordinate system as the MR coordinate system.\n",
    "- `bem_solution` contains information about how currents propagate through the three media, brain, skull and skin, to the electrodes and to the magnetometers and gradiometers. The entry in `[i, j]` indicates how much the potential, [*V*] changes in `i` when a unit current dipole (1 Am) is placed in `j`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1a1c2-82a6-40fd-b6db-e04b3f4d3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFO ABOUT WHERE CHANNELS ARE (in subject's head space)\n",
    "info = epochs_sample.info\n",
    "print(info['chs'][2]['loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59241a47-9229-4b06-9d05-5a6a6b8728a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFO ABOUT WHERE SOURCES ARE (in subject's MR space)\n",
    "src = fwd['src']\n",
    "# src = mne.source_space.setup_source_space(subject='sample', subjects_dir=subjects_dir)\n",
    "print(fwd['src'][0]['rr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03b7b3-239d-4e13-b7ff-89e14857f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSFORMATION MATRIX THAT MAPS THE TWO SPACES ONTO ANOTHER\n",
    "trans = mne.read_trans('sample_audvis_raw-trans.fif')\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b5b7f-cd6c-4d75-a6ef-a4721d6a83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOUNDARY ELEMENT METHOD\n",
    "\n",
    "## describe the surfaces and their conductivities\n",
    "#bem_model = mne.bem.make_bem_model(subject=subject, subjects_dir=subjects_dir,\n",
    "#                           conductivity=[0.3, 0.006, 0.3]) ## three layer model\n",
    "\n",
    "## model how electrical potentials spread to the electrodes and how the \n",
    "# currents of the brain are related to the magnetic field measured at the\n",
    "# sensors\n",
    "#bem_solution = mne.bem.make_bem_solution(bem_model)\n",
    "\n",
    "bem_solution = mne.bem.read_bem_solution('../../subjects/sample/bem/sample-5120-5120-5120-bem-sol.fif')\n",
    "\n",
    "print(bem_solution['solution'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a13e14-be99-4e42-b0b4-d6757407a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a forward solution using four ingredients; L(r),\n",
    "# which maps how sources in the brain link to sensors in the helmet:\n",
    "# info: information about channel positions and sensor types\n",
    "# trans: the transformation needed to align channel positions with MR\n",
    "# src: the source model (in this case, a cortical sheet)\n",
    "# bem_solution: modelling of how electrical currents spread toward the\n",
    "#               electrodes   \n",
    "\n",
    "#fwd = mne.make_forward_solution(info=info, trans=trans, src=src, bem=bem_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5160c-2d12-4cca-9e54-74ea6d620d26",
   "metadata": {},
   "source": [
    "## FINALLY DO THE MINIMUM NORM ESTIMATE\n",
    "\n",
    "$$\n",
    "\\huge \\hat{\\boldsymbol \\nu}_{vox}(t) = \\boldsymbol L_V^T(\\boldsymbol G + \\epsilon \\boldsymbol I)^{-1} \\boldsymbol b(t)    \n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\huge \\boldsymbol G = \\int_\\Omega \\boldsymbol L (\\boldsymbol r) \\boldsymbol L^T (\\boldsymbol r) d^3r\n",
    "$$\n",
    "and with\n",
    "$$ \n",
    "\\huge\n",
    "\\boldsymbol{\\hat{\\nu}}_{vox}(t) = \\left[\n",
    "\\begin{array}{c} \n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_1, t) \\\\\n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_2, t) \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_N, t)\n",
    "\\end{array}\n",
    "\\right]  \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489e909-170a-4f4c-a989-730b16e3430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  used to whiten the data, i.e. normalizing magnetometers, gradiometers\n",
    "## and electrode readings to make them comparable\n",
    "noise_cov = mne.compute_covariance(epochs_sample, tmin=None, tmax=0)\n",
    "noise_cov.plot(raw_sample.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a1c20-05dc-4508-922d-7e85c7dc7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_operator = mne.minimum_norm.make_inverse_operator(epochs_sample.info, fwd,\n",
    "                                                          noise_cov)\n",
    "\n",
    "# estimating the source pattern for each time point Vvox(t)\n",
    "# right auditory stimulus (evokeds_sample[1])\n",
    "MNE = mne.minimum_norm.apply_inverse(evoked_sample, inverse_operator,\n",
    "                                     method='MNE')\n",
    "## standard is to use method=dSPM for depth correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc056f-8db3-4642-a2f4-86f5aea2d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNE.plot(subject='sample', subjects_dir=subjects_dir, hemi='both') ## can only be done locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53dd75-1d81-44d8-8a9e-484df1144d8e",
   "metadata": {},
   "source": [
    "## EXTRA ASSIGNMENTS\n",
    "- Compare the `'dSPM` method with the `'MNE'` method in `.apply_inverse()`. How do the source reconstructions differ?\n",
    "- Do you see any sub-cortical or cerebellar sources? Why (not)?\n",
    "- Plot the difference between conditions `'LA'` and `'RA'` on the source level - where are the main differences? To subtract two source time courses from one another you can use:\n",
    "```\n",
    "MNE_diff = MNE_LA.copy()\n",
    "MNE_diff._data -= MNE_RA.data\n",
    "```\n",
    "- Also plot the difference between conditions `'LV'` and `'RV'`\n",
    "- What sense can you make of `plt.imshow(bem_solution['solution'][:20, :20], clim=(0, 1e-4))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f7ecf-868d-40cb-8eea-a5f9ad415770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
