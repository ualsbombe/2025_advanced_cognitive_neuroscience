{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff05b627-2042-4f1a-8c60-2b9c37ff3600",
   "metadata": {},
   "source": [
    "# TUTORIAL W36\n",
    "\n",
    "In this tutorial, we will get acquainted with working with MEG data on UCloud  \n",
    "We will be using the MNE-sample-data that contains visual and auditory stimulations  \n",
    "The dataset also contains anatomical MR-data processed with FreeSurfer, meaning that we have access to a forward model linking the source space to sensor space.  \n",
    "With that we can create an inverse model for reconstructing the sensor space visual and auditory responses in the source space.\n",
    "\n",
    "**Make sure that you have followed the instructions for setting up your environment, local and UCloud before beginning on this.** The instructions can be found here: https://github.com/ualsbombe/2025_advanced_cognitive_neuroscience/blob/main/README.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af465a9-8e7a-476a-8fd5-b5b9b7ac5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import mne ## MNE-Python for analysing data\n",
    "## below magic provides interactive plots in notebook\n",
    "%matplotlib widget\n",
    "from os import chdir\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt ## for basic plotting\n",
    "import matplotlib as mpl ## for setting default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c2855-0808-44c4-97ee-bf424131712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SET DEFAULT PLOTTING PARAMETERS\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams['font.size'] = 24\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "mpl.rcParams['lines.linewidth'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dcdea-7107-4e6c-9463-3ae58bf5f055",
   "metadata": {},
   "source": [
    "## SAMPLE DATA SET (https://mne.tools/stable/documentation/datasets.html#sample)\n",
    "*These data were acquired with the Neuromag Vectorview system at MGH/HMS/MIT Athinoula A. Martinos Center Biomedical Imaging. EEG data from a 60-channel electrode cap was acquired simultaneously with the MEG. The original MRI data set was acquired with a Siemens 1.5 T Sonata scanner using an MPRAGE sequence.*\n",
    "\n",
    "*In this experiment, checkerboard patterns were presented to the subject into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.*\n",
    "\n",
    "Change the path to your relevant path below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc3952-af5b-464c-9c98-7b63470b3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LOAD SAMPLE DATA SET\n",
    "\n",
    "sample_path = '/work/MEG_data/MNE-sample-data' ## UCloud\n",
    "sample_path = '/home/lau/mne_data/MNE-sample-data/' ## local\n",
    "sample_meg_path = join(sample_path, 'MEG', 'sample')\n",
    "chdir(sample_meg_path)\n",
    "subjects_dir = '../../subjects/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94eb06d-2c27-4ae3-bb48-c4afa208f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% READ RAW\n",
    "\n",
    "raw_sample = mne.io.read_raw_fif('sample_audvis_raw.fif', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b9cc4-92cd-4a88-8d19-155ea561081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT RAW DATA\n",
    "fig = raw_sample.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab1575-71e3-41ab-84b5-939726dbc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE POWER SPECTRAL DENSITY\n",
    "psd = raw_sample.compute_psd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913692f-8200-439f-8bc9-2e3623b7c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT POWER SPECTRAL DENSITY\n",
    "fig = psd.plot() # online low-pass filter of 170 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb74ab-809a-4f77-acb5-74a8595ee8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTERING DATA WITH LOW-PASS FILTER\n",
    "raw_sample.filter(l_freq=None, h_freq=40) # low-pass filter of 40 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787db9c-f64e-4ef8-8873-309fc4fddf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTTING POWER SPECTRAL DENSITY AFTER LOW-PASS FILTER\n",
    "fig = raw_sample.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81e98d-61d6-459b-8c3b-72e4981b6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% FIND EVENTS\n",
    "\n",
    "events_sample = mne.find_events(raw_sample)\n",
    "mne.viz.plot_events(events_sample)\n",
    "mne.viz.plot_events(events_sample, sfreq=raw_sample.info['sfreq']);\n",
    "\n",
    "## LA: 1: Response to left-ear auditory stimulus (a tone)\n",
    "## RA: 2: Response to right-ear auditory stimulus\n",
    "## LV: 3: Response to left visual field stimulus (checkerboard)\n",
    "## RV: 4: Response to right visual field stimulus\n",
    "## smiley: 5: Response to the smiley face\n",
    "## button: 32: Response triggered by the button press\n",
    "# https://mne.tools/stable/overview/datasets_index.html#sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8197e-05ff-4dd7-9708-d0b1d0fdc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% EPOCH THE DATA\n",
    "event_id = dict(LA=1) ## we'll just look at this one event\n",
    "tmin = -0.200 # s\n",
    "tmax =  0.600 # s\n",
    "baseline = (None, 0) # s (from beginning to 0); for demeaning\n",
    "\n",
    "## segment the data\n",
    "epochs_sample = mne.Epochs(raw_sample, events_sample,\n",
    "                           event_id, tmin, tmax, baseline, preload=True)\n",
    "epochs_sample.set_eeg_reference(projection=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b5f8c-3dd8-466b-a154-36b167b7aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT EPOCHS\n",
    "fig = epochs_sample.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29ff86-786d-4c35-8cef-51f5ddea0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% AVERAGE (EVOKED)\n",
    "evoked_sample = epochs_sample.average()\n",
    "print(evoked_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff464c7-1054-4f55-8240-9ecf25f96962",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTTING AVERAGES\n",
    "evoked_sample.plot() \n",
    "mne.viz.plot_evoked_topo(evoked_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f2ee0-1a8b-4931-9c4f-9480540d50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HAVING MORE THAN ONE EVENT\n",
    "\n",
    "event_id = dict(LA=1, RA=2, LV=3, RV=4)\n",
    "more_epochs = mne.Epochs(raw_sample, events_sample,\n",
    "                           event_id, tmin=-0.200, tmax=0.600,\n",
    "                           baseline=(None, 0),\n",
    "                           preload=True)\n",
    "\n",
    "more_evokeds = list()\n",
    "for event in more_epochs.event_id:\n",
    "    more_evokeds.append(more_epochs[event].average());\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac67266-bae9-4342-bfac-5e330cd22cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT SEVERAL EVENTS\n",
    "mne.viz.plot_evoked_topo(more_evokeds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7c278-5754-49f8-8dff-40874e0251f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ FORWARD MODEL\n",
    "fwd = mne.read_forward_solution('sample_audvis-meg-eeg-oct-6-fwd.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc6928-33f0-47e2-ad1d-569c2d95ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT SOURCE SPACE\n",
    "\n",
    "# extract source space\n",
    "src = fwd['src']\n",
    "##plot alignment does not work on UCloud\n",
    "mne.viz.plot_alignment(info=epochs_sample.info, src=src, subjects_dir='../../subjects/',\n",
    "                       subject=\"sample\", surfaces=\"white\", trans='sample_audvis_raw-trans.fif')\n",
    "\n",
    "## 2D alternative\n",
    "\n",
    "#mne.viz.plot_sensors(raw_sample.info);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1a1c2-82a6-40fd-b6db-e04b3f4d3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFO ABOUT WHERE CHANNELS ARE (in subject's head space)\n",
    "info = epochs_sample.info\n",
    "print(info['chs'][2]['loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59241a47-9229-4b06-9d05-5a6a6b8728a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFO ABOUT WHERE SOURCE ARE (in subject's MR space)\n",
    "src = fwd['src']\n",
    "# src = mne.source_space.setup_source_space(subject='sample', subjects_dir=subjects_dir)\n",
    "print(fwd['src'][0]['rr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03b7b3-239d-4e13-b7ff-89e14857f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSFORMATION MATRIX THAT MAPS THE TWO SPACES ONTO ANOTHER\n",
    "trans = mne.read_trans('sample_audvis_raw-trans.fif')\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b5b7f-cd6c-4d75-a6ef-a4721d6a83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOUNDARY ELEMENT METHOD\n",
    "\n",
    "## describe the surfaces and their conductivities\n",
    "#bem_model = mne.bem.make_bem_model(subject=subject, subjects_dir=subjects_dir,\n",
    "#                           conductivity=[0.3, 0.006, 0.3]) ## three layer model\n",
    "\n",
    "## model how electrical potentials spread to the electrodes and how the \n",
    "# currents of the brain are related to the magnetic field measured at the\n",
    "# sensors\n",
    "#bem_solution = mne.bem.make_bem_solution(bem_model)\n",
    "\n",
    "bem_solution = mne.bem.read_bem_solution('../../subjects/sample/bem/sample-5120-5120-5120-bem-sol.fif')\n",
    "\n",
    "print(bem_solution['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a13e14-be99-4e42-b0b4-d6757407a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a forward solution using four ingredients; L(r),\n",
    "# which maps how sources in the brain link to sensors in the helmet:\n",
    "# info: information about channel positions and sensor types\n",
    "# trans: the transformation needed to align channel positions with MR\n",
    "# src: the source model (in this case, a cortical sheet)\n",
    "# bem_solution: modelling of how electrical currents spread toward the\n",
    "#               electrodes   \n",
    "\n",
    "#fwd = mne.make_forward_solution(info=info, trans=trans, src=src, bem=bem_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489e909-170a-4f4c-a989-730b16e3430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## again used to whiten the data, i.e. normalizing magnetometers, gradiometers\n",
    "## and electrode readings to make them comparable\n",
    "noise_cov = mne.compute_covariance(epochs_sample, tmin=None, tmax=0)\n",
    "noise_cov.plot(raw_sample.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a1c20-05dc-4508-922d-7e85c7dc7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_operator = mne.minimum_norm.make_inverse_operator(epochs_sample.info, fwd,\n",
    "                                                          noise_cov)\n",
    "\n",
    "# estimating the source pattern for each time point Vvox(t)\n",
    "# right auditory stimulus (evokeds_sample[1])\n",
    "MNE = mne.minimum_norm.apply_inverse(evoked_sample, inverse_operator,\n",
    "                                     method='MNE')\n",
    "## standard is to use method=dSPM for depth correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc056f-8db3-4642-a2f4-86f5aea2d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNE.plot(subject='sample', subjects_dir=subjects_dir, hemi='both')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
