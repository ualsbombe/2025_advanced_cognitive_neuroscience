{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15f9bff",
   "metadata": {},
   "source": [
    "## TUTORIAL W40\n",
    "\n",
    "In W38 we started this tutorial about how to analyse visual responses.\n",
    "Steps included:  \n",
    "- preprocessing\n",
    "     - artefact rejection\n",
    "     - filtering\n",
    "- epoching\n",
    "     - rejecting based on peak-to-peak amplitude\n",
    "- evoked responses\n",
    "      - difference waves\n",
    "- estimating noise covariance\n",
    "      - whitening the data\n",
    "- do an inverse model (MNE)\n",
    "- extract labels from the cerebral cortex\n",
    "    - plot source times courses from different labels\n",
    "    - do difference plots\n",
    "- do a multivariate analysis in source space\n",
    "    - (doing it in sensor space is left as an exercise to the reader)\n",
    " \n",
    "You can either finish the tutorial on the sample data set or start applying it to the workshop data.\n",
    "The old code is left in; but (mostly) commented out\n",
    "\n",
    "Recommended Gb=12, 1 CPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d81679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS AND DEFAULT PLOTTING PARAMETERS\n",
    "\n",
    "import mne ## MNE-Python for analysing data\n",
    "## below magic provides interactive plots in notebook\n",
    "%matplotlib widget\n",
    "from os import chdir\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt ## for basic plotting\n",
    "import matplotlib as mpl ## for setting default parameters\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e3c3c",
   "metadata": {},
   "source": [
    "## SAMPLE DATA SET (https://mne.tools/stable/documentation/datasets.html#sample)\n",
    "*These data were acquired with the Neuromag Vectorview system at MGH/HMS/MIT Athinoula A. Martinos Center Biomedical Imaging. EEG data from a 60-channel electrode cap was acquired simultaneously with the MEG. The original MRI data set was acquired with a Siemens 1.5 T Sonata scanner using an MPRAGE sequence.*\n",
    "\n",
    "*In this experiment, checkerboard patterns were presented to the subject into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.*\n",
    "\n",
    "Change the path to your relevant path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LOAD SAMPLE DATA SET\n",
    "\n",
    "#sample_path = '/work/MEG_data/MNE-sample-data' ## UCloud\n",
    "#sample_meg_path = join(sample_path, 'MEG', 'sample')\n",
    "#chdir(sample_meg_path)\n",
    "#subjects_dir = '../../subjects/'\n",
    "\n",
    "#%% LOAD YOUR FAVOURITE SUBJECT\n",
    "\n",
    "MEG_path = '/work/MEG_data/workshop_data/0163/20250922_000000' ## you can pick your study group member\n",
    "subjects_dir = '/work/freesurfer'\n",
    "behaviour_path = '/work/MEG_data/workshop_data/behavioural_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d36da3",
   "metadata": {},
   "source": [
    "## INSPECT RAW DATA\n",
    "\n",
    "### NEW INFO - workshop data\n",
    "\n",
    "Note how plotting it will seem meaningless; it is all saturated. Do a PSD to find out why\n",
    "\n",
    "### OLD INFO - sample data\n",
    "\n",
    "First try to identify the two bad channels, one electrode and one planar gradiometer.  \n",
    "(They have been greyed out; also notice that you can mark channels as bad by clicking them and \"unbadding\" them by clicking it again)\n",
    "Do notice that the two marked channels look considerably different than the others  \n",
    "The bad channels can also be found by `raw.info['bads']`\n",
    "The x-axis has time and the y-axes, magnetic field (T), magnetic gradient (T/m) or voltage (V)\n",
    "This is your $\\boldsymbol b(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% READ RAW\n",
    "\n",
    "#raw = mne.io.read_raw_fif('sample_audvis_raw.fif', preload=True)\n",
    "raw = mne.io.read_raw_fif(join(MEG_path, 'workshop_2025_raw.fif'), preload=True) \n",
    "fig = raw.plot() ## you will note this is completely saturated (all blue) do the PSD and find out why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9e737",
   "metadata": {},
   "source": [
    "## IDENTIFYING HPI-FREQUENCIES FROM THE POWER SPECTRAL DENSITY (PSD)\n",
    "\n",
    "### NEW INFO\n",
    "\n",
    "The X-axis contains the frequency (Hz) and the y-axis the power of each for each frequency in dB. Here you can also click and toggle bad channels.\n",
    "What frequencies do the HPI run at, do you think, and would that necessitate a filter?\n",
    "\n",
    "### OLD INFO\n",
    "\n",
    "Now we will see the two bad channels in the power spectral density.\n",
    "Describe to yourself how they differ from the rest. Often bad channels are easier to identify here than in the raw traces\n",
    "The X-axis contains the frequency (Hz) and the y-axis the power of each for each frequency in dB. Here you can also click and toggle bad channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.compute_psd().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a34e7",
   "metadata": {},
   "source": [
    "## FILTERING\n",
    "\n",
    "### NEW CODE\n",
    "\n",
    "You would want to do something about those HPI-frequencies saturating the signal\n",
    "\n",
    "### OLD CODE\n",
    "\n",
    "With filtering, we can reduce the contributions of frequencies that contain signal that is not of interest to our analysis.\n",
    "For each of the copies, compute a psd and ascertain for yourself what they do and not do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4724872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% CHOOSE A FILTER TO APPLY TO YOUR RAW DATA - \n",
    "\n",
    "raw.filter(h_freq=??, l_freq=??); ## this method \".filter()\" modifies in the object in place\n",
    "\n",
    "#copy_lowpass = raw.copy() ## create a copy so we do not overwrite the original\n",
    "#copy_lowpass.filter(h_freq=40,   l_freq=None) ## lowpass filter of 40 Hz\n",
    "\n",
    "#copy_highpass = raw.copy()\n",
    "#copy_highpass.filter(h_freq=None, l_freq=1) ## highpass filter of 1 Hz\n",
    "\n",
    "#copy_bandstop = raw.copy()\n",
    "#copy_bandstop.filter(h_freq=1,    l_freq=40) ## bandstop filler of 1-40 Hz\n",
    "\n",
    "#copy_bandpass = raw.copy()\n",
    "#copy_bandpass.filter(h_freq=40,   l_freq=1); ## bandpass fillter of 1-40 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6421aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## what does it look after choosing an appropriate filter?\n",
    "raw.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2049b9e",
   "metadata": {},
   "source": [
    "## FIND THE EVENTS\n",
    "\n",
    "Pick the ones that you want to analyse further - you might get an error about *min_duration*; if you do, set that to 0.002 (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b25f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding events on: STI101\n",
      "1600 events found on stim channel STI101\n",
      "Event IDs: [ 1  3  4  6  8 10 12 14]\n"
     ]
    }
   ],
   "source": [
    "#%% FIND EVENTS\n",
    "\n",
    "events = mne.find_events(raw)\n",
    "\n",
    "# Check the set_experiment parameters methods in the class Experiment\n",
    "## https://github.com/ualsbombe/2025_advanced_cognitive_neuroscience/blob/main/experiment/subjective_experience_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e11cc1-8b9c-493a-bf25-d50b5d6f9400",
   "metadata": {},
   "source": [
    "## CHANGE EVENTS\n",
    "\n",
    "If you want to change some of the events, this is also possible. `events` is an `(n_events, 3)` array, with the first column, having the time stamps of the event and the third column having the trigger codes.\n",
    "Please also not that the trial number inadventenly has not been recorded; and all columns from all trial number should be moved one column to the right. *subjective_response* is thus really under *objective_response_time_ms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(join(behaviour_path, '0163_2025_09_22_144412_experiment_data.csv'), index_col=1)\n",
    "print(behaviour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6393",
   "metadata": {},
   "source": [
    "## CREATING EPOCHS\n",
    "\n",
    "\n",
    "### NEW CODE\n",
    "\n",
    "Keep it simple and create epochs without `reject`\n",
    "Choose a meaningful `tmin` and `tmax` and a meaningful `baseline` (the response time of subjects could inform `tmin` and `tmax`)\n",
    "Also include relevant events using `event_id` as  a `dict`\n",
    "\n",
    "\n",
    "### OLD CODE\n",
    "Create epochs using `mne.Epochs`. Specifically, create two epochs_objects: `epochs` and `epochs_eog_cleaned`.\n",
    "- In both:\n",
    "    - include 200 ms before each visual event and 550 ms after each event.\n",
    "    - make sure to include both checkerboard stimuli epochs\n",
    "    - do the demeaning by creating a baseline interval from 200 ms to 0 ms.\n",
    "    - apply the `set_eeg_reference(projection=True)` to both\n",
    "- In `epochs_eog_cleaned`:\n",
    "    - include a `reject` dict that removes epochs where the peak-to-peak amplitude of eye-related activity (EOG) exceeds 250 µV\n",
    "Check out https://mne.tools/stable/generated/mne.Epochs.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW CODE\n",
    "\n",
    "epochs = mne.Epochs(??)\n",
    "\n",
    "\n",
    "## OLD CODE\n",
    "\n",
    "#epochs = mne.Epochs(?)\n",
    "#epochs_eog_cleaned = mne.Epochs(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08708fa3",
   "metadata": {},
   "source": [
    "## CREATE AVERAGES AND DIFFERENCE WAVES\n",
    "\n",
    "It would be worth checking what the difference waves look like for the evoked responses associated with different levels of subjective experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88878d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING AVERAGES AND DIFFERENCE WAVES\n",
    "\n",
    "### NEW CODE\n",
    "\n",
    "evokeds = list()\n",
    "for event in epochs.event_id:\n",
    "    evokeds.append(epochs[event].average())\n",
    "\n",
    "## this might help you along\n",
    "evoked_diffs = list()\n",
    "diffs = ['name_1', 'name_2', ...]\n",
    "for diff in diffs:\n",
    "    evoked_diff = evokeds[??].copy() # create a copy\n",
    "    evoked_diff.data -= evokeds[??].data # modify the data in place\n",
    "    evoked_diff.comment = diff\n",
    "\n",
    "\n",
    "### OLD CODE\n",
    "\n",
    "#evokeds = list()\n",
    "#evokeds_eog_cleaned = list()\n",
    "#for event in epochs.event_id:\n",
    "#    evokeds.append(epochs[event].average())\n",
    "#    evokeds_eog_cleaned.append(epochs_eog_cleaned[event].average())\n",
    "\n",
    "#evoked_diff = evokeds[0].copy() # create a copy\n",
    "#evoked_diff.data -= evokeds[1].data # modify the data in place\n",
    "#evoked_diff.comment = 'LV - RV'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b21c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4bd4d33",
   "metadata": {},
   "source": [
    "## PLOT THE EVOKEDS AND THEIR DIFFERENCE WAVES\n",
    "Find out where the differences are the greatest - what does this indicate\n",
    "Use `mne.viz.plot_evoked`, `mne.viz.plot_evoked_topo`, `mne.viz.plot_evoked_topomap`, `mne.viz.plot_evoked_joint`, `mne.viz.plot_evoked_image` to get a feel for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4271410",
   "metadata": {},
   "source": [
    "## FORWARD MODEL\n",
    "\n",
    "$\\boldsymbol L (\\boldsymbol r)$ is our forward model that for each source location $\\boldsymbol r$, expresses how that source is linked to the sensors. The SI-unit is $\\frac {T} {Am}$.  \n",
    "The SI-unit for the magnetic field at each time point, $t$, $\\boldsymbol b (t)$ is $T$.  \n",
    "The SI-unit for the current density $\\boldsymbol s(\\boldsymbol r, t)$  at each location $\\boldsymbol r$ and time point $t$ is $Am$.  \n",
    "The forward model states for each source at whatever $\\boldsymbol r$ how its activation in $Am$ is linked to the magnetic field at each sensor, e.g. $b_1(t)$.  \n",
    "$\\boldsymbol n(t)$ is the Gaussian noise at each time point\n",
    "\n",
    "$\\boldsymbol b(t) = \\left[\n",
    "\\begin{array}{c} \n",
    "b_1(t) \\\\\n",
    "b_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "b_M(t)\n",
    "\\end{array}\n",
    "\\right]$  \n",
    "$\\boldsymbol{b}(t) = \\boldsymbol{L}(\\boldsymbol{r}) \\boldsymbol s(\\boldsymbol r, t) + \\boldsymbol n(t)$\n",
    "\n",
    "### NEW CODE\n",
    "\n",
    "Two of the ingredients have been created for you. The `bem` and the `src`\n",
    "\n",
    "\n",
    "### OLD CODE\n",
    "\n",
    "The nice people from MNE-Python have already made a forward model for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ad558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET FORWARD MODEL\n",
    "\n",
    "### NEW CODE\n",
    "\n",
    "info = evoked.info\n",
    "trans = join(MEG_path, ?subject? + '-trans.fif') ## this, we don't have yet - we will create this together\n",
    "src = join(subjects_dir, ?subject?, 'bem', ?subject? + '-oct-6-src.fif')\n",
    "bem = join(subjects_dir, ?subject?, 'bem', ?subject? + '-5120-bem-sol.fif')\n",
    "\n",
    "fwd = mne.make_forward_solution(\n",
    "                            info, trans,\n",
    "                            src, bem)\n",
    "\n",
    "\n",
    "### OLD CODE\n",
    "\n",
    "#fwd = mne.read_forward_solution('sample_audvis-meg-eeg-oct-6-fwd.fif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08259d",
   "metadata": {},
   "source": [
    "##  THE MINIMUM NORM ESTIMATE\n",
    "\n",
    "$$\n",
    "\\huge \\hat{\\boldsymbol \\nu}_{vox}(t) = \\boldsymbol L_V^T(\\boldsymbol G + \\epsilon \\boldsymbol I)^{-1} \\boldsymbol b(t)    \n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\huge \\boldsymbol G = \\int_\\Omega \\boldsymbol L (\\boldsymbol r) \\boldsymbol L^T (\\boldsymbol r) d^3r\n",
    "$$\n",
    "and with\n",
    "$$ \n",
    "\\huge\n",
    "\\boldsymbol{\\hat{\\nu}}_{vox}(t) = \\left[\n",
    "\\begin{array}{c} \n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_1, t) \\\\\n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_2, t) \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol{\\hat s} (\\boldsymbol r_N, t)\n",
    "\\end{array}\n",
    "\\right]  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668ec0e",
   "metadata": {},
   "source": [
    "## CREATE SOURCE TIME COURSES FOR EACH CONDITION\n",
    "\n",
    "### NEW CODE\n",
    "\n",
    "Create `stc` (`stc = mne.minimum_norm.apply_inverse`) from `evokeds` , separate ones for each event and maybe also one collapsed across events\n",
    "- For the noise covariance plots, which of the two kinds of sensors show the highest covariance between sensors? And does that make sense?\n",
    "\n",
    "\n",
    "### OLD CODE\n",
    "\n",
    "Create `stc` (`stc = mne.minimum_norm.apply_inverse`) from `evokeds` , separate ones for each event ((LV and RV))\n",
    "- Do you think you would see more frontal activity in `stc` compared to `stc_eog_cleaned`? And why is that?\n",
    "- For the noise covariance plots, which of three kinds of sensors show the highest correlations between sensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### NEW CODE\n",
    "\n",
    "##  WHITEN the data, i.e. normalizing magnetometers, gradiometers\n",
    "## and electrode readings to make them comparable\n",
    "noise_cov = mne.compute_covariance(epochs, tmin=None, tmax=0)\n",
    "noise_cov.plot(raw.info)\n",
    "\n",
    "inverse_operator = mne.minimum_norm.make_inverse_operator(epochs.info, fwd,\n",
    "                                                          noise_cov)\n",
    "\n",
    "# estimating the source pattern for each time point Vvox(t)\n",
    "\n",
    "stc = mne.minimum_norm.apply_inverse(evoked=??, method='dSPM') # dSPM makes a depth correction\n",
    "#Dale AM, Liu AK, Fischl BR, et al (2000) Dynamic Statistical Parametric Mapping: Combining fMRI and MEG for High-Resolution Imaging of Cortical Activity. Neuron 26:55–67. https://doi.org/10.1016/S0896-6273(00)81138-1\n",
    "\n",
    "\n",
    "### OLD CODE\n",
    "\n",
    "##  WHITEN the data, i.e. normalizing magnetometers, gradiometers\n",
    "## and electrode readings to make them comparable\n",
    "#noise_cov = mne.compute_covariance(epochs, tmin=None, tmax=0)\n",
    "#noise_cov.plot(raw.info)\n",
    "\n",
    "#inverse_operator = mne.minimum_norm.make_inverse_operator(epochs.info, fwd,\n",
    "#                                                          noise_cov)\n",
    "\n",
    "# estimating the source pattern for each time point Vvox(t)\n",
    "# use the MNE method and not the default dSPM\n",
    "#stc = mne.minimum_norm.apply_inverse(method='MNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffec4b",
   "metadata": {},
   "source": [
    "## EXTRACT RELEVANT LABELS FROM THE CEREBRAL CORTEX\n",
    "\n",
    "go to your subject label path: and read some relevant labels using `mne.read_label` from there:\n",
    " - old path `MNE-sample-data/subjects/sample/label`, \n",
    " - new_path `/work/freesurfer/?subject?/label\n",
    "\n",
    "\n",
    "That could be:\n",
    " - `lh.V1.label`\n",
    " - `lh.V2.label`\n",
    " - `rh.V1.label`\n",
    " - `rh.V2.label`\n",
    "\n",
    "You can read in even more labels by running `labels = mne.read_labels_from_annot('?subject?, subjects_dir=subjects_dir)`\n",
    "\n",
    "Plot some (visual) labels that should show a response, using `plt.plot` from `matplotlib`. Here's some code that could get you started:\n",
    "\n",
    "```\n",
    "lh_V1_label = mne.read_label(<path_to_lh_V1_label>)\n",
    "stc_lh_V1_label = stc.in_label(lh_V1_label)\n",
    "plt.figure()\n",
    "plt.plot(stc_lh_V1_label.times, stc_lh_V1_label.data.T)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Current density (Am)')\n",
    "plt.title(lh_V1_label.name)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "Also plot some labels that should not show a response (maybe some language related labels)\n",
    "Discuss with each other what the lines each represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8642e",
   "metadata": {},
   "source": [
    "##  MACHINE LEARNING LOGISTIC REGRESSION \n",
    "\n",
    "Finally, we are going to do some machine learning on this data, trying to predict whether the checkerboard was shown in the left visual field or the right visual field. For this we need to do source reconstruction of the epochs instead.\n",
    "\n",
    "\n",
    "Below is some code to get you started - **this is the old code; please adapt to your analysis**\n",
    "\n",
    "```\n",
    "\n",
    "## GETTING SOURCE TIME COURSES FROM EPOCHS\n",
    "\n",
    "stcs = mne.minimum_norm.apply_inverse_epochs(epochs, lambda=1, method='MNE')\n",
    "\n",
    "## READING LABELS FROM ANNOTATION\n",
    "labels = mne.read_labels_from_annot('sample', subjects_dir=subjects_dir)\n",
    "\n",
    "## EXTRACT LABELS FROM MULTIPLE STCS\n",
    "def extract_label(label, stcs):\n",
    "    label_stcs = list()\n",
    "    for stc in stcs:\n",
    "        label_stcs.append(stc.in_label(label))\n",
    "        \n",
    "    return label_stcs\n",
    "\n",
    "\n",
    "#%% CREATE X AND Y FOR LOGISTIC REGRESSIOM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = epochs.events[:, 2] # the codes for the visual events\n",
    "\n",
    "n_events = len(y) ## how many repetitions\n",
    "n_samples = label_stcs[0].data.shape[1] # how many time points\n",
    "n_label_vertices = label_stcs[0].data.shape[0]  # how many source positions\n",
    "\n",
    "X = np.zeros(shape=(n_events, n_label_vertices, n_samples))\n",
    "\n",
    "## assign data to X\n",
    "for event_index in range(n_events):\n",
    "    X[event_index, :, :] = label_stcs[event_index].data\n",
    "    \n",
    "    \n",
    "#%% DO LOGISTIC REGRESSION PER TIME SAMPLE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scores_list_samples = [None] * n_samples\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "for sample_index in range(n_samples):\n",
    "    print(sample_index)\n",
    "    this_X = X[:, :, sample_index]\n",
    "    this_X_std = sc.fit_transform(this_X) ## standardise the data\n",
    "    logr = LogisticRegression(C=1e-3)\n",
    "    scores_list_samples[sample_index] = np.mean(cross_val_score(logr,\n",
    "                                                                this_X_std,\n",
    "                                                                y,\n",
    "                                                        cv=StratifiedKFold()))\n",
    "\n",
    "\n",
    "## PLOTTING THE CLASSIFICATION ACCURACY\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stc.times, scores_list_samples)\n",
    "plt.show()\n",
    "plt.title('Classification of LV vs RV')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Proportion correctly classif')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806397c0",
   "metadata": {},
   "source": [
    "## THINKING AHEAD - QUESTIONS YOU MIGHT WANT TO PONDER ON\n",
    "\n",
    " - How to conduct a group analysis? (You could get inspired below)\n",
    "    - Andersen LM (2018) Group Analysis in MNE-Python of Evoked Responses from a Tactile Stimulation Paradigm: A Pipeline for Reproducibility at Every Step of Processing, Going from Individual Sensor Space Representations to an across-Group Source Space Representation. Frontiers in Neuroscience 12:\n",
    "    - Andersen LM (2023) Software and Resources for Experiments and Data Analysis. In: Grimaldi M, Brattico E, Shtyrov Y (eds) Language Electrified: Principles, Methods, and Future Perspectives of Investigation. Springer US, New York, NY, pp 65–122\n",
    " - What to do about physiological signals?\n",
    "   - Independent Component Analysis?\n",
    "   - Reject trials with physiological signals?\n",
    "   - Leave them in and use beamformer instead of MNE?\n",
    " - In the multivariate analyses, what to do about the differences in contrasts per trial?\n",
    "   - Regress contrast differences out?\n",
    "      - Barnett B, Andersen LM, Fleming SM, Dijkstra N (2024) Identifying content-invariant neural signatures of perceptual vividness. PNAS Nexus 3:pgae061. https://doi.org/10.1093/pnasnexus/pgae061\n",
    " - Do a behavioural analysis\n",
    "   - Do we see differences in response times and accuracy between different levels of PAS?\n",
    "   - Create some code that cleans up the column mess in the log file (don't change the log itself)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
